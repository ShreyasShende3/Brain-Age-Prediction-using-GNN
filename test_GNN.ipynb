{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba04c86b-714a-4668-8d5e-333aa54a2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f49bc9-ba10-4a73-9474-ba0e11452dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ages shape: (587,)\n",
      "Brain characteristics shape: (2, 587, 360)\n",
      "Connectivity shape: (587, 360, 360)\n",
      "Number of unique ages: 410\n"
     ]
    }
   ],
   "source": [
    "print(\"Ages shape:\", ages.shape)\n",
    "print(\"Brain characteristics shape:\", brain_characteristics.shape)\n",
    "print(\"Connectivity shape:\", connectivity.shape)\n",
    "print(\"Number of unique ages:\", len(np.unique(ages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b43e956-298b-4767-8e34-80d97ba3e529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1947.2012, Val Loss: 1045.7918\n",
      "Epoch 2, Train Loss: 757.2323, Val Loss: 446.6912\n",
      "Epoch 3, Train Loss: 695.2520, Val Loss: 481.8288\n",
      "Epoch 4, Train Loss: 577.7260, Val Loss: 477.8602\n",
      "Epoch 5, Train Loss: 581.4380, Val Loss: 430.8341\n",
      "Epoch 6, Train Loss: 620.1038, Val Loss: 477.0501\n",
      "Epoch 7, Train Loss: 605.3998, Val Loss: 514.4586\n",
      "Epoch 8, Train Loss: 621.5810, Val Loss: 589.5982\n",
      "Epoch 9, Train Loss: 611.6444, Val Loss: 419.7325\n",
      "Epoch 10, Train Loss: 630.2848, Val Loss: 413.6206\n",
      "Epoch 11, Train Loss: 605.7658, Val Loss: 452.1151\n",
      "Epoch 12, Train Loss: 665.5902, Val Loss: 504.2598\n",
      "Epoch 13, Train Loss: 575.3516, Val Loss: 414.7028\n",
      "Epoch 14, Train Loss: 611.6040, Val Loss: 403.9059\n",
      "Epoch 15, Train Loss: 599.1761, Val Loss: 545.0967\n",
      "Epoch 16, Train Loss: 680.9378, Val Loss: 442.9479\n",
      "Epoch 17, Train Loss: 537.1679, Val Loss: 452.5381\n",
      "Epoch 18, Train Loss: 505.3424, Val Loss: 424.0148\n",
      "Epoch 19, Train Loss: 510.6759, Val Loss: 411.8120\n",
      "Epoch 20, Train Loss: 590.4283, Val Loss: 484.0054\n",
      "Epoch 21, Train Loss: 604.4589, Val Loss: 479.4070\n",
      "Epoch 22, Train Loss: 592.3665, Val Loss: 438.4683\n",
      "Epoch 23, Train Loss: 583.3307, Val Loss: 388.2204\n",
      "Epoch 24, Train Loss: 570.5107, Val Loss: 475.9942\n",
      "Epoch 25, Train Loss: 590.5315, Val Loss: 381.6572\n",
      "Epoch 26, Train Loss: 559.0798, Val Loss: 384.8810\n",
      "Epoch 27, Train Loss: 702.3117, Val Loss: 728.5650\n",
      "Epoch 28, Train Loss: 559.7907, Val Loss: 395.3605\n",
      "Epoch 29, Train Loss: 577.1055, Val Loss: 389.2536\n",
      "Epoch 30, Train Loss: 510.9350, Val Loss: 379.2938\n",
      "Epoch 31, Train Loss: 516.1868, Val Loss: 394.2422\n",
      "Epoch 32, Train Loss: 600.1223, Val Loss: 509.5527\n",
      "Epoch 33, Train Loss: 531.5716, Val Loss: 385.3559\n",
      "Epoch 34, Train Loss: 513.8452, Val Loss: 376.5139\n",
      "Epoch 35, Train Loss: 519.0112, Val Loss: 362.2796\n",
      "Epoch 36, Train Loss: 505.7989, Val Loss: 390.4770\n",
      "Epoch 37, Train Loss: 450.2494, Val Loss: 345.7579\n",
      "Epoch 38, Train Loss: 636.9331, Val Loss: 355.8806\n",
      "Epoch 39, Train Loss: 511.9051, Val Loss: 386.4820\n",
      "Epoch 40, Train Loss: 504.2188, Val Loss: 463.9067\n",
      "Epoch 41, Train Loss: 577.5709, Val Loss: 376.5670\n",
      "Epoch 42, Train Loss: 595.0641, Val Loss: 489.9071\n",
      "Epoch 43, Train Loss: 499.0624, Val Loss: 351.5948\n",
      "Epoch 44, Train Loss: 499.3258, Val Loss: 369.7070\n",
      "Epoch 45, Train Loss: 516.2118, Val Loss: 318.7157\n",
      "Epoch 46, Train Loss: 562.2654, Val Loss: 344.9324\n",
      "Epoch 47, Train Loss: 483.6209, Val Loss: 321.4129\n",
      "Epoch 48, Train Loss: 493.5644, Val Loss: 316.7273\n",
      "Epoch 49, Train Loss: 474.0236, Val Loss: 274.8505\n",
      "Epoch 50, Train Loss: 419.2806, Val Loss: 230.2993\n",
      "Test MAE: 12.0315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "# Load files\n",
    "age_labels = np.load(\"Downloads/DL_Project/age_labels_592_sbj_filtered.npy\")[:587]  # Adjust to the smallest dimension\n",
    "features = np.load(\"Downloads/DL_Project/cam_can_thicks_myelins_tensor_592_filtered.npy\")[:, :587, :]  # Match samples\n",
    "connectivity = np.load(\"Downloads/DL_Project/plv_tensor_592_sbj_filtered.npy\")[:587, :, :]\n",
    "brain_regions = pd.read_csv(\"Downloads/DL_Project/HCP-MMP1_UniqueRegionList.csv\")\n",
    "\n",
    "# Transpose features to match (num_samples, num_regions, num_features)\n",
    "features = np.transpose(features, (1, 2, 0))  # Shape: (587, 360, 2)\n",
    "\n",
    "# Create edges and weights\n",
    "def create_edges(plv_tensor):\n",
    "    avg_connectivity = np.mean(plv_tensor, axis=0)  # Shape: (360, 360)\n",
    "    row, col = np.where(avg_connectivity > 0)  # Extract non-zero connections\n",
    "    edge_weights = avg_connectivity[row, col]\n",
    "    edge_index = torch.tensor([row, col], dtype=torch.long)  # Shape: (2, num_edges)\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)  # Shape: (num_edges,)\n",
    "    return edge_index, edge_weights\n",
    "    \n",
    "edge_weights = (edge_weights - edge_weights.mean()) / edge_weights.std()\n",
    "edge_index, edge_weights = create_edges(connectivity)\n",
    "node_features = torch.tensor(features, dtype=torch.float)\n",
    "labels = torch.tensor(age_labels, dtype=torch.float)\n",
    "\n",
    "# Prepare PyTorch Geometric dataset\n",
    "dataset = []\n",
    "for i in range(len(labels)):\n",
    "    data = Data(x=node_features[i], edge_index=edge_index, edge_attr=edge_weights, y=labels[i])\n",
    "    dataset.append(data)\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 2: Define the GNN Model\n",
    "class WeightedHyperbolicGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, edge_dim=1)\n",
    "        self.gat2 = GATConv(hidden_dim, hidden_dim, edge_dim=1)\n",
    "        self.gat3 = GATConv(hidden_dim, hidden_dim, edge_dim=1)  # New Layer\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = torch.relu(self.gat1(x, edge_index, edge_attr))\n",
    "        x = torch.relu(self.gat2(x, edge_index, edge_attr))\n",
    "        x = torch.relu(self.gat3(x, edge_index, edge_attr))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Step 3: Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WeightedHyperbolicGNN(input_dim=2, hidden_dim=64, output_dim=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).squeeze()\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).squeeze()\n",
    "            loss = loss_fn(pred, batch.y)\n",
    "            total_loss += loss.item()\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            truths.extend(batch.y.cpu().numpy())\n",
    "    return total_loss / len(loader), preds, truths\n",
    "\n",
    "for epoch in range(50):  # Adjust epochs as needed\n",
    "    train_loss = train(train_loader)\n",
    "    val_loss, _, _ = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Step 4: Test the model and compute MAE\n",
    "_, preds, truths = evaluate(test_loader)\n",
    "mae = mean_absolute_error(truths, preds)\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cefd7fe-3a8a-4e95-bfcd-21c835295872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Adam and SmoothL1Loss\n",
      "Epoch 1, Val Loss: 0.5321, Val MAE: 0.9597\n",
      "Epoch 2, Val Loss: 0.5590, Val MAE: 0.9737\n",
      "Epoch 3, Val Loss: 0.4262, Val MAE: 0.8209\n",
      "Epoch 4, Val Loss: 0.4644, Val MAE: 0.8594\n",
      "Epoch 5, Val Loss: 0.3739, Val MAE: 0.7593\n",
      "Epoch 6, Val Loss: 0.2855, Val MAE: 0.6250\n",
      "Epoch 7, Val Loss: 0.3038, Val MAE: 0.6654\n",
      "Epoch 8, Val Loss: 0.2868, Val MAE: 0.6266\n",
      "Epoch 9, Val Loss: 0.3191, Val MAE: 0.6852\n",
      "Epoch 10, Val Loss: 0.2210, Val MAE: 0.5472\n",
      "\n",
      "Training with AdamW and MSELoss\n",
      "Epoch 1, Val Loss: 1.5766, Val MAE: 1.0431\n",
      "Epoch 2, Val Loss: 3.3631, Val MAE: 1.5983\n",
      "Epoch 3, Val Loss: 1.4247, Val MAE: 0.9806\n",
      "Epoch 4, Val Loss: 0.5368, Val MAE: 0.5885\n",
      "Epoch 5, Val Loss: 0.4805, Val MAE: 0.5078\n",
      "Epoch 6, Val Loss: 0.4998, Val MAE: 0.5307\n",
      "Epoch 7, Val Loss: 0.5043, Val MAE: 0.5378\n",
      "Epoch 8, Val Loss: 0.4766, Val MAE: 0.5124\n",
      "Epoch 9, Val Loss: 0.4752, Val MAE: 0.5128\n",
      "Epoch 10, Val Loss: 0.4711, Val MAE: 0.5151\n",
      "\n",
      "Final Results:\n",
      "Experiment 1 MAE: 0.5472\n",
      "Experiment 2 MAE: 0.5151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Data Preprocessing with Advanced Normalization\n",
    "# Load files\n",
    "age_labels = np.load(\"Downloads/DL_Project/age_labels_592_sbj_filtered.npy\")[:587]\n",
    "features = np.load(\"Downloads/DL_Project/cam_can_thicks_myelins_tensor_592_filtered.npy\")[:, :587, :]\n",
    "connectivity = np.load(\"Downloads/DL_Project/plv_tensor_592_sbj_filtered.npy\")[:587, :, :]\n",
    "brain_regions = pd.read_csv(\"Downloads/DL_Project/HCP-MMP1_UniqueRegionList.csv\")\n",
    "\n",
    "# Advanced Normalization\n",
    "age_scaler = StandardScaler()\n",
    "normalized_ages = age_scaler.fit_transform(age_labels.reshape(-1, 1)).squeeze()\n",
    "\n",
    "# Normalize features per subject and per feature dimension\n",
    "features = np.transpose(features, (1, 2, 0))  # Shape: (587, 360, 2)\n",
    "normalized_features = np.zeros_like(features, dtype=float)\n",
    "for i in range(features.shape[0]):\n",
    "    for j in range(features.shape[2]):\n",
    "        normalized_features[i, :, j] = StandardScaler().fit_transform(features[i, :, j].reshape(-1, 1)).squeeze()\n",
    "\n",
    "# Create edges and weights with robust connectivity calculation\n",
    "def create_edges(plv_tensor):\n",
    "    avg_connectivity = np.mean(plv_tensor, axis=0)\n",
    "    threshold = np.percentile(avg_connectivity[avg_connectivity > 0], 50)  # Dynamic thresholding\n",
    "    row, col = np.where(avg_connectivity >= threshold)\n",
    "    edge_weights = avg_connectivity[row, col]\n",
    "    \n",
    "    # Normalize edge weights\n",
    "    edge_weights = (edge_weights - edge_weights.mean()) / (edge_weights.std() + 1e-7)\n",
    "    \n",
    "    edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    return edge_index, edge_weights\n",
    "\n",
    "edge_index, edge_weights = create_edges(connectivity)\n",
    "\n",
    "# Prepare PyTorch Geometric dataset\n",
    "dataset = []\n",
    "for i in range(len(normalized_ages)):\n",
    "    node_features = torch.tensor(normalized_features[i], dtype=torch.float)\n",
    "    label = torch.tensor(normalized_ages[i], dtype=torch.float)\n",
    "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weights, y=label)\n",
    "    dataset.append(data)\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# 2. Advanced GNN Model with Multiple Optimizations\n",
    "class RobustWeightedHyperbolicGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Graph Attention Layers\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=4, concat=True, edge_dim=1)\n",
    "        self.gat2 = GATConv(hidden_dim * 4, hidden_dim * 2, heads=4, concat=True, edge_dim=1)\n",
    "        self.gat3 = GATConv(hidden_dim * 8, hidden_dim, heads=2, concat=False, edge_dim=1)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_dim * 8)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = torch.nn.Dropout(p=0.3)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # First GAT layer\n",
    "        x = self.gat1(x, edge_index, edge_attr)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.gat2(x, edge_index, edge_attr)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third GAT layer\n",
    "        x = self.gat3(x, edge_index, edge_attr)\n",
    "\n",
    "        # Global Mean Pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 3. Device and Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RobustWeightedHyperbolicGNN(input_dim=2, hidden_dim=64, output_dim=1).to(device)\n",
    "\n",
    "# 4. Multiple Loss Functions and Optimizers\n",
    "# Huber Loss for robustness\n",
    "loss_fn1 = torch.nn.SmoothL1Loss()  # Huber Loss\n",
    "loss_fn2 = torch.nn.MSELoss()\n",
    "\n",
    "# Multiple Optimizers\n",
    "optimizer1 = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "optimizer2 = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning Rate Schedulers\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1, 'min', patience=3, factor=0.5)\n",
    "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer2, T_max=10)\n",
    "\n",
    "# 5. Training Function with Multiple Strategies\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn, scheduler, epochs=10):\n",
    "    print(f\"\\nTraining with {type(optimizer).__name__} and {type(loss_fn).__name__}\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).squeeze()\n",
    "            loss = loss_fn(pred, batch.y)\n",
    "            \n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_truths = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).squeeze()\n",
    "                val_loss += loss_fn(pred, batch.y).item()\n",
    "                val_preds.extend(pred.cpu().numpy())\n",
    "                val_truths.extend(batch.y.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_mae = mean_absolute_error(val_truths, val_preds)\n",
    "        \n",
    "        # Learning Rate Scheduling\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    return val_mae\n",
    "\n",
    "# 6. Comparative Experiments\n",
    "experiments = [\n",
    "    (model, train_loader, val_loader, optimizer1, loss_fn1, scheduler1),\n",
    "    (model, train_loader, val_loader, optimizer2, loss_fn2, scheduler2)\n",
    "]\n",
    "\n",
    "results = []\n",
    "for exp in experiments:\n",
    "    # Reset model weights before each experiment\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "    mae = train_and_evaluate(*exp)\n",
    "    results.append(mae)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for i, mae in enumerate(results, 1):\n",
    "    print(f\"Experiment {i} MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98985f26-b6b1-439b-9b2c-c00c9af93cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
